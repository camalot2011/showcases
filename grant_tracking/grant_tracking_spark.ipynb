{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NIH Grant Tracking with Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase I: Develop and Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I will explore the NIH grant tracking project in the spark framework. First of all, let's run some development process with a smaller data set from the grant abstracts residing in the local `Data/PRJABS/` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "import os\n",
    "import dill\n",
    "import re\n",
    "from tqdm import tqdm_notebook\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem.porter import *\n",
    "sns.set()\n",
    "matplotlib.rcParams['figure.dpi'] = 144"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the spark section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "sc = SparkContext(\"local[*]\", \"temp\")\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the local file path conversion function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def localpath(path):\n",
    "    return 'file://' + os.path.join(os.path.abspath(os.path.curdir), path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare a test folder with the grant abstracts in 1985:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shutil import copyfile\n",
    "if os.path.exists('Data/test/'):\n",
    "    os.system('rm -r Data/test')\n",
    "    os.mkdir('Data/test/')\n",
    "    fns = os.listdir('Data/PRJABS/')\n",
    "    for fn in sorted(fns)[:3]:\n",
    "        copyfile(os.path.join('Data/PRJABS/',fn),\n",
    "                 os.path.join('Data/test/',fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the abstract content in the test folder into spark, clean text, tokenize the corpus, and stem the words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|     Id|               words|\n",
      "+-------+--------------------+\n",
      "|3091032|[the, primari, de...|\n",
      "|3104974|[the, asthma, and...|\n",
      "|3091425|[thi, program, pr...|\n",
      "|3104981|[the, research, t...|\n",
      "|3091436|[the, host-parasi...|\n",
      "+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "abstract = sc.textFile(localpath('Data/test/'))\n",
    "def text_cleaning(doc):\n",
    "    '''lower case, clean words/symbols'''\n",
    "    rm_list ='\\\"|\\,|\\(|  +|\\)|\\.|\\'|\\:'\n",
    "    doc = re.sub(r'{}'.format(rm_list),' ',doc)\n",
    "    doc = doc.strip().lower()\n",
    "    return doc\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "def stem(words):\n",
    "    '''Get the stem of the words'''\n",
    "    words_stem = []\n",
    "    for word in words:\n",
    "        word_stem = stemmer.stem(word)\n",
    "        if len(word_stem) > 2:\n",
    "            words_stem.append(word_stem)\n",
    "    return words_stem\n",
    "\n",
    "df_abs = (abstract.map(lambda doc: text_cleaning(doc))\n",
    "                  .filter(lambda doc: len(doc) > 0)\n",
    "                  .filter(lambda line: not line.startswith('app'))\n",
    "                  .map(lambda doc: doc.split(' '))\n",
    "                  .map(lambda word: [x for x in word if len(x)>0])\n",
    "                  .map(lambda word: stem(word))\n",
    "                  .map(lambda doc: (int(doc[0]), doc[1:]))\n",
    "                  .filter(lambda doc: len(doc[1])>0)\n",
    "                  .toDF(['Id','words']))\n",
    "df_abs.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the machine learning pipeline with the LDA model on spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import CountVectorizer, Tokenizer, IDF\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.clustering import LDA\n",
    "from nltk.corpus import stopwords\n",
    "# build the pipeline and lda model with online optimizer\n",
    "stop_words = StopWordsRemover(inputCol='words',\n",
    "                             outputCol='clean')\n",
    "stop_words.setStopWords(stop_words.loadDefaultStopWords('english'))\n",
    "countv = CountVectorizer(inputCol=stop_words.getOutputCol(), \n",
    "                         outputCol=\"tokens\")\n",
    "idf = IDF(inputCol=countv.getOutputCol(),outputCol=\"features\")\n",
    "lda = LDA(maxIter=10,k=10,optimizer='online')\n",
    "\n",
    "pipeline = Pipeline(stages=[stop_words, countv, idf, lda])\n",
    "\n",
    "lda_model = pipeline.fit(df_abs)\n",
    "\n",
    "labels = lda_model.transform(df_abs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.387032246261784"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.stages[3].logPerplexity(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify the label as the topic with the max probability\n",
    "# save the label to file\n",
    "if os.path.exists('Data/Topics/'):\n",
    "    os.system('rm -r Data/Topics/')\n",
    "topic_labels = (labels.select('Id','topicDistribution')\n",
    "                      .rdd\n",
    "                      .map(lambda x: (x[0],np.argmax(x[1])))\n",
    "                      .saveAsTextFile(localpath('Data/Topics/labels/')))\n",
    "# Get the topics\n",
    "wordnum = 5 # choose the number of topic words\n",
    "vocabulary = lda_model.stages[1].vocabulary\n",
    "# make the vocabulary the broadcast variable to all the nodes\n",
    "# convert the topics from indices to actual words and store them\n",
    "voc_bv = sc.broadcast(vocabulary)\n",
    "topic_df =(lda_model.stages[3].describeTopics(wordnum)\n",
    "                    .rdd\n",
    "                    .map(lambda x:(x[0],[voc_bv.value[Id] for Id in x[1]],x[2]))\n",
    "                    .saveAsTextFile(localpath('Data/Topics/words/')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the scripts on the local `Hadoop` file system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!hadoop fs -rm -r hdfs:///user/vagrant/output/\n",
    "!spark-submit --master local[*] --py-files myutils.py grant.py \\\n",
    "    hdfs:///user/vagrant/test/ hdfs:///user/vagrant/output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'output': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!rm -r output\n",
    "!mkdir output\n",
    "!hadoop fs -copyToLocal hdfs:///user/vagrant/output/* output/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase II: Run the whole LDA learning on GCP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following commands need to be implemented in the **terminal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the output folder if it already exists\n",
    "!gsutil rm -r gs://camalot/output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to make sure that the `cluster_init.sh` has the necessary packages including `numpy` and `ntlk`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more powerful\n",
    "!gcloud dataproc clusters create cluster-1 --initialization-actions \\\n",
    "    gs://camalot/cluster_init.sh --region us-west2 --zone us-west2-a \\\n",
    "    --master-machine-type n1-standard-2 --master-boot-disk-size 500GB \\\n",
    "    --num-workers 3 \\\n",
    "    --worker-machine-type n1-standard-4 --worker-boot-disk-size 500GB \\\n",
    "    --num-worker-local-ssds 1 --project tdi2018-217422"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud dataproc jobs submit pyspark --cluster cluster-1 --region us-west2 \\\n",
    "    --py-files gs://camalot/datacourse/grant_tracking/myutils.py \\\n",
    "    gs://camalot/datacourse/grant_tracking/grant.py \\\n",
    "    -- gs://camalot/data/NIH/PRJABS/ \\\n",
    "            gs://camalot/output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shut down cluster at the end\n",
    "!gcloud dataproc clusters delete cluster-1 --region us-west2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
